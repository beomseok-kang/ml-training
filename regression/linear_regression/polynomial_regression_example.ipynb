{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71422a4e",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "Polynomial regression can be considered as a linear regression. The non-linearity of regression model is determined by the regression coefficients' linearity, not the linearity of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd7974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일차 단항식 계수 피처:\n",
      " [[0 1]\n",
      " [2 3]]\n",
      "변환된 2차 다항식 계수 피처:\n",
      " [[1. 0. 1. 0. 0. 1.]\n",
      " [1. 2. 3. 4. 6. 9.]]\n"
     ]
    }
   ],
   "source": [
    "#  Use PolynomialFeatures to change the features into polynomial features.\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "# change [x1, x2] to [1, x1, x2, x1^2, x1x2, x2^2]\n",
    "X = np.arange(4).reshape(2, 2)\n",
    "print('일차 단항식 계수 피처:\\n', X)\n",
    "\n",
    "# degree = 2, to change to quadratic (polynomial with degree of 2) formula\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly.fit(X)\n",
    "poly_ftr = poly.transform(X)\n",
    "print('변환된 2차 다항식 계수 피처:\\n', poly_ftr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd3c0a",
   "metadata": {},
   "source": [
    "for \\[x1, x2\\], when \\[x1, x2\\] = \\[0, 1\\], \\[1, x1, x2, x1^2, x1x2, x2^2\\] = \\[1, 0, 1, 0, 0, 1\\], and when \\[x1, x2\\] = \\[2, 3\\], \\[1, x1, x2, x1^2, x1x2, x2^2\\] = \\[1, 2, 3, 4, 6, 9\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ff332",
   "metadata": {},
   "source": [
    "### Example \\#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be0d6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일차 단항식 계수 feature:\n",
      " [[0 1]\n",
      " [2 3]]\n",
      "삼차 다항식 결정값:\n",
      " [  5 125]\n"
     ]
    }
   ],
   "source": [
    "# Make sample polynomial regression, of y = 1 + 2x1 + 3x1^2 + 4x2^3\n",
    "\n",
    "def polynomial_func(X):\n",
    "    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n",
    "    return y\n",
    "\n",
    "X = np.arange(4).reshape(2, 2)\n",
    "print('일차 단항식 계수 feature:\\n', X)\n",
    "y = polynomial_func(X)\n",
    "print('삼차 다항식 결정값:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdacf4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3차 다항식 계수 feature:\n",
      " [[ 1.  0.  1.  0.  0.  1.  0.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.  8. 12. 18. 27.]]\n",
      "Polynomial 회귀 계수\n",
      " [0.   0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34]\n",
      "Polynomial 회귀 Shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# change to polynomial of degree 3\n",
    "poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)\n",
    "print('3차 다항식 계수 feature:\\n', poly_ftr)\n",
    "\n",
    "# Create LinearRegression model and find the regression coefficients of each\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(poly_ftr, y)\n",
    "print('Polynomial 회귀 계수\\n', np.round(model.coef_, 2))\n",
    "print('Polynomial 회귀 Shape:', model.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab488a",
   "metadata": {},
   "source": [
    "### Example \\#2 (Same, but using pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c00217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial 회귀 계수\n",
      " [0.   0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "def polynomial_func(X):\n",
    "    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3\n",
    "    return y\n",
    "\n",
    "# Create Pipeline\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
    "                 ('linear', LinearRegression())])\n",
    "X = np.arange(4).reshape(2, 2)\n",
    "y = polynomial_func(X)\n",
    "\n",
    "model = model.fit(X, y)\n",
    "\n",
    "print('Polynomial 회귀 계수\\n', np.round(model.named_steps['linear'].coef_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221330a8",
   "metadata": {},
   "source": [
    "## Polynomial Regression's Overfitting and Underfitting Problem\n",
    "\n",
    "Although a high degree polynomial regression can model a very complex regression model, highering the degree of the polynomial regression would increase the risk of the model overfitting to the train dataset, which could lower the test dataset's prediction evaluation metrics.\n",
    "\n",
    "![Example of overfitting/underfitting](https://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png)\n",
    "\n",
    "[Link](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)\n",
    "\n",
    "\n",
    "You can see from the graphs that when the degree is too low, the model is underfitted. When the degree is too high, the model is overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601e397",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade Off\n",
    "\n",
    "Often there exists a trade off relationship between bias and variance. Underfitting happens when bias is high but variance is low, and overfitting happens when bias is low but variance is high.\n",
    "\n",
    "![Bias/Variance](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png)\n",
    "Illustration of bias and variance\n",
    "\n",
    "![Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "Tradeoff between bias and variance\n",
    "\n",
    "Hence, the key of this trade-off is to find the optimal model complexity point where the error minimises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03495812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
